public:: false
alias:: 相互独立

- TODO 在概率论和统计学中，随机变量的独立性是一个非常重要的概念，它描述了不同随机变量之间不存在任何概率上的关联性。如果两个或多个随机变量相互独立，那么这些变量中任何一个的取值都不会影响其他变量的概率分布。这一点在统计分析、概率计算、数据建模等领域中都有着极其重要的作用。
- ### 定义
- 随机变量 \(X\) 和 \(Y\) 被称为相互独立，如果对于所有的 \(x\) 和 \(y\)，事件 \(X = x\) 和事件 \(Y = y\) 是独立的，即它们同时发生的概率等于各自发生的概率的乘积：
- $$P(X = x \text{ 和 } Y = y) = P(X = x) \times P(Y = y)$$
- 这个定义也可以推广到两个以上的随机变量。对于随机变量 \(X_1, X_2, \ldots, X_n\)，它们是相互独立的，如果对于所有可能的取值 \(x_1, x_2, \ldots, x_n\)，有：
- $$P(X_1 = x_1 \text{ 和 } X_2 = x_2 \text{ 和 } \ldots \text{ 和 } X_n = x_n) = P(X_1 = x_1) \times P(X_2 = x_2) \times \ldots \times P(X_n = x_n)$$
- ### 性质
	- **函数的独立性**：
	  logseq.order-list-type:: number
		- 如果随机变量 \(X\) 和 \(Y\) 是独立的，那么它们的任何函数 \(g(X)\) 和 \(h(Y)\) 也是独立的。
	- **和与积的分布**：
	  logseq.order-list-type:: number
		- 如果 \(X\) 和 \(Y\) 是独立的，那么它们的和 \(X+Y\) 或积 \(X \times Y\) 的分布可以通过 \(X\) 和 \(Y\) 的边缘分布来找出。
	- **协方差和相关系数**：
	  logseq.order-list-type:: number
		- 如果两个随机变量独立，则它们的协方差为零（协方差为零不一定意味着独立）。
		- 相应地，它们的相关系数也为零。
- ### 示例
- 假设有两个随机变量 \(X\) 和 \(Y\)，其中 \(X\) 投掷一枚公平硬币的结果（正面为1，反面为0），\(Y\) 投掷一个公平六面骰子的结果。因为投掷硬币的结果不会影响投掷骰子的结果，因此 \(X\) 和 \(Y\) 是相互独立的。